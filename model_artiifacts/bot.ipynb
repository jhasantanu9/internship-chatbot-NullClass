{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab29e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e92d43",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ec643",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5abf1f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NullClassChatbot:\n",
    "    def __init__(self, intents_file, confidence_threshold=0.5):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.intents = None\n",
    "        self.responses = {}\n",
    "        self.load_intents(intents_file)\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def augment_data(self, texts, labels):\n",
    "        augmented_texts = []\n",
    "        augmented_labels = []\n",
    "        \n",
    "        for text, label in zip(texts, labels):\n",
    "            # Add original text\n",
    "            augmented_texts.append(text)\n",
    "            augmented_labels.append(label)\n",
    "            \n",
    "            # Add slightly modified versions\n",
    "            tokens = word_tokenize(text)\n",
    "            if len(tokens) > 3:\n",
    "                # Remove one random word\n",
    "                removed_word = random.randint(0, len(tokens)-1)\n",
    "                new_text = ' '.join(tokens[:removed_word] + tokens[removed_word+1:])\n",
    "                augmented_texts.append(new_text)\n",
    "                augmented_labels.append(label)\n",
    "                \n",
    "                # Shuffle word order slightly\n",
    "                if len(tokens) > 4:\n",
    "                    shuffled = tokens.copy()\n",
    "                    idx = random.randint(0, len(tokens)-2)\n",
    "                    shuffled[idx], shuffled[idx+1] = shuffled[idx+1], shuffled[idx]\n",
    "                    augmented_texts.append(' '.join(shuffled))\n",
    "                    augmented_labels.append(label)\n",
    "        \n",
    "        return augmented_texts, augmented_labels\n",
    "    \n",
    "    def load_intents(self, intents_file):\n",
    "        with open(intents_file, 'r') as f:\n",
    "            self.intents = json.load(f)['intents']\n",
    "            \n",
    "        # Create mapping of intents to responses\n",
    "        for intent in self.intents:\n",
    "            self.responses[intent['intent']] = intent['responses']\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        for intent in self.intents:\n",
    "            for example in intent['examples']:\n",
    "                texts.append(self.preprocess_text(example))\n",
    "                labels.append(intent['intent'])\n",
    "        \n",
    "        # Convert labels to numerical format\n",
    "        numerical_labels = self.label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Augment training data\n",
    "        augmented_texts, augmented_labels = self.augment_data(texts, numerical_labels)\n",
    "        \n",
    "        return augmented_texts, augmented_labels\n",
    "    \n",
    "    def train(self, epochs=10, batch_size=16, learning_rate=2e-5):\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        texts, labels = self.prepare_data()\n",
    "        \n",
    "        # Split data into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = IntentDataset(X_train, y_train, self.tokenizer)\n",
    "        val_dataset = IntentDataset(X_val, y_val, self.tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels=len(self.label_encoder.classes_)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize optimizer with weight decay\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.1, patience=2, verbose=True\n",
    "        )\n",
    "        \n",
    "        best_val_accuracy = 0\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    labels = batch['label'].to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    \n",
    "                    val_loss += outputs.loss.item()\n",
    "                    \n",
    "                    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            \n",
    "            val_accuracy = 100 * correct / total\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            print(f'Average training loss: {total_loss / len(train_loader):.4f}')\n",
    "            print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "            print(f'Validation accuracy: {val_accuracy:.2f}%')\n",
    "            print('--------------------')\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    def predict(self, text):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model hasn't been trained yet!\")\n",
    "        \n",
    "        # Preprocess input text\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        \n",
    "        self.model.eval()\n",
    "        encoding = self.tokenizer(\n",
    "            processed_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "            \n",
    "            if confidence.item() < self.confidence_threshold:\n",
    "                return None, confidence.item()\n",
    "            \n",
    "            predicted_intent = self.label_encoder.inverse_transform([predicted_class.item()])[0]\n",
    "            return predicted_intent, confidence.item()\n",
    "    \n",
    "    def get_response(self, text):\n",
    "        intent, confidence = self.predict(text)\n",
    "        \n",
    "        if intent is None:\n",
    "            return \"I'm not quite sure I understand. Could you please rephrase your question?\"\n",
    "        \n",
    "        print(f\"Detected intent: {intent} (confidence: {confidence:.2f})\")  # Debug info\n",
    "        responses = self.responses[intent]\n",
    "        return random.choice(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb1bee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize and train the chatbot\n",
    "    chatbot = NullClassChatbot('intents.json', confidence_threshold=0.4)\n",
    "    print(\"Training the chatbot...\")\n",
    "    chatbot.train(epochs=10, batch_size=8)  \n",
    "    \n",
    "    # Interactive loop\n",
    "    print(\"\\nNullClass Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            response = chatbot.get_response(user_input)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chatbot: I'm sorry, I encountered an error. Please try again.\")\n",
    "            print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70192904",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
